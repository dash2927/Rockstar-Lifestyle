{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 51219,\n",
      "  \"iopub_port\": 51220,\n",
      "  \"stdin_port\": 51221,\n",
      "  \"control_port\": 51222,\n",
      "  \"hb_port\": 51223,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"2a5341fd-eab1fe0c9c3ed1697f531062\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-847fd546-7f17-4756-8cfe-44e7b7af3dc7.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "     <img src=\"https://raw.githubusercontent.com/hhelmbre/Rockstar-Lifestyle/master/doc/Logo.png\" width=\"19%\" align=\"left\">\n",
    " \n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Neural Network Using Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The folling jupyter notebook will use neural networks in order to get quantifiable data for given images. It uses the neuralnet.py python file and imcrop.py image manipulation python file\n",
    "\n",
    "---\n",
    "\n",
    "The following functions will be used:\n",
    "\n",
    "* neuralnet( dataset = [], NN_settings = None, train = False, save_settings = True, print_acc = False)\n",
    "    * Description: This function is a wrapper function that will use a neural network in order to best estimate the protein count of the input images. The solver used in this neural network is L-BFGS which is a quasi-Newton method that is theoretically and experimentally verified to have a faster convergence and works well with low-dimensional input. L-BFGS does not work well in other settings because of its high memory requirement and lack of use of minibatches. There are other methods that can be tried (Adam, SGD BatchNorm , Adadelta, Adagrad), but L-BFGS was the best choice for this application and for the time constraint given in producing this package.\n",
    "    * Parameters: \n",
    "    \n",
    "        - dataset: dataset to use neural network on\n",
    "        - NN_settings: classifier data\n",
    "        - train: decide on whether to train or test\n",
    "        - save_settings: decide on whether to save classifier data to 'data' folder\n",
    "        - print_acc: passes bool to accuracy() to tell function to print accuracy\n",
    "    \n",
    "    * Output:\n",
    "        - count: The counts of the input dataset\n",
    "    \n",
    "    \n",
    "* accuracy( test_x, test_y, classifier, output = False)\n",
    "    * Description: This function checks the accuracy of the neural network by sending testing data through the classifier and outputing the accuracy\n",
    "    * Parameters: \n",
    "    \n",
    "        - test_x: testing data\n",
    "        - test_y: answered data\n",
    "        - classifier: classifier object set by MLPClassifier\n",
    "        - output: boolean operator to determine whether to output info or not\n",
    "\n",
    "    * Output:\n",
    "        - acc: accuracy of the neural network\n",
    "    \n",
    "    \n",
    "* create_train_set(numb_sets=None, prev_set=None, bin_list=None, gauss_blur_list=None)\n",
    "    * Description: This function creates a training set of n datasets. If a previous set is provided, the function will add to the previous set and output a backup set along with the changed set.\n",
    "    \n",
    "     *Note: This function is weak, the image formation is not up to par and will be changed in the future to provide a better training set for the neural network. Right now, the training sets output only protein counts between 5600 and 5800 proteins which will not train the neural network properly.\n",
    "     \n",
    "    * Parameters: \n",
    "    \n",
    "        - numb_sets: number of training sets to create\n",
    "        - prev_set: list of training objs that usr might have\n",
    "        - bin_list: list of bins to use in MRH calculations\n",
    "        - gauss_blur_list: list of gauss blur vars to use\n",
    "    \n",
    "    * Output:\n",
    "        - prev_set: list of training data\n",
    "        - backup: list of backup training data in case new set is unacceptable\n",
    "\n",
    "* load_objects(name='untitled.dat')\n",
    "    * Description: This function will load an object-based dataset using dill which is a version of pickle (a function that stores large amounts of object data) into a subfolder\n",
    "    * Parameters: \n",
    "    \n",
    "        - name: name of filename to use, default = untitled.dat\n",
    "    \n",
    "    * Output:\n",
    "        - output: list of objects\n",
    "\n",
    "* _loadall(dir_par_path, name)\n",
    "    * Description: This function function is a private function for load_objects that will load a generator for the file name in the given path\n",
    "    * Parameters: \n",
    "    \n",
    "        - dir_par_path: folder path of the given file\n",
    "    \n",
    "    * Output:\n",
    "        - generator: generator of the loaded file\n",
    "          or\n",
    "          [] if directory doesn't exist\n",
    "\n",
    "* save_objects(dataset, name='untitled.dat')\n",
    "    * Description: This function will save an object set using dill which is a version of pickle (a function that stores large amounts of object data) into a subfolder named 'data'.\n",
    "    \n",
    "     *Note: If only saving a single object, put in a [list] of len 1 i.e. save_objects([obj], name = 'file_name')\n",
    "     \n",
    "    * Parameters: \n",
    "    \n",
    "        - dataset: array of objects to save\n",
    "        - name: name of file pickle will be saved to\n",
    "\n",
    "The following objects will be used:\n",
    "\n",
    "* TestObj()\n",
    "    * Description: Object for test and training values\n",
    "    * Contains:\n",
    "        - res : image resolution\n",
    "        - data : image data\n",
    "        - MRH : Multi Resolution Histogram data for image\n",
    "        - GB : Gaussian blur data for image\n",
    "        - bin_list : bin list for corresponding MRH\n",
    "        - heights : heights from MRH difference\n",
    "        - count : number of objects in image\n",
    "    \n",
    "* NNSettings()\n",
    "    * Description: Stores settings for neural network, mostly used for sgd; will be used more for future projects\n",
    "    * Contains:\n",
    "        - classifier : classifier object\n",
    "        - learn_rate : learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before starting on the example, there is one thing to note: pickling is a dangerous method to use, the data used in this package is data which is simply compressed into ASCII text via pickling. This makes it so that anyone can adjust these parameters and access python os commands maliciously. For this demo,we will use pickling, but in the future we will look into other solutions for storing large sets of data.\n",
    "\n",
    "In addition to this, it is useful to note that these files are very large and people who wish to contribute or manipulate files will need to download and use [Git Large File Storage](https://git-lfs.github.com/). This is a pretty easy extension to use:\n",
    "1. Download the extension from the link provided\n",
    "2. Go to desired master branch\n",
    "3. Execute the following commands:\n",
    "\n",
    "    i. Set up git lfs\n",
    "    ```console\n",
    "    git lfs install \n",
    "    ```\n",
    "    ii. Select type of file to track\n",
    "    ```console\n",
    "    git lfs track \"*.dat\"\n",
    "    ```\n",
    "    iii. Make sure .gitattributes is tracked\n",
    "    ```console\n",
    "    git add .gitattributes \n",
    "    ```\n",
    "\n",
    "And thats it, git will automatically use lfs on all .dat files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rockstarlifestyle import neuralnet as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_settings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object=5, black=6\n"
     ]
    }
   ],
   "source": [
    "obj_normalized = 5\n",
    "black = 6\n",
    "print('object=' + \n",
    "\t\t\t\t str(obj_normalized) + \n",
    "\t\t\t\t ', black=' + \n",
    "\t\t\t\t str(black))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-71e29b43851f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m warnings.warn(\"File saved does not end in '.dat', will \"\\\n\u001b[0;32m      2\u001b[0m                                          \u001b[1;34m\"have problems reading. Automatically \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \t\t\t\t\t \"renaming to '{}'.\".format(name))\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "warnings.warn(\"File saved does not end in '.dat', will \"\\\n",
    "\t\t\t\t\t \"have problems reading. Automatically \"\\\n",
    "\t\t\t\t\t \"renaming to '{}'.\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
